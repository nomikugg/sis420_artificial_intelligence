{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Primer Parcial (Regresion logistica One vs All) \n",
    " <h3>Primer Parcial Ejercicio D1 (Clasificacion one-vs-all) <h3>\n",
    " <HR>\n",
    " <h3>\n",
    "  NOMBRE: Rojas Arroyo Luis Fernando <br>\n",
    "  CARRERA: INGENIERIA DE SISTEMAS <BR>\n",
    "  FECHA: 09/04/2024 <BR>\n",
    "\n",
    "  <!-- * [Enlace de invitacion para ser colaborador](https://github.com/bspoloo/SIS420-012024/invitations)\n",
    "  \n",
    "  * [Enlace al git hub](https://github.com/bspoloo/SIS420-012024/tree/main/Laboratorios/Laboratorio%203)\n",
    "  \n",
    "  * [Enlace al Colab](https://colab.research.google.com/github/bspoloo/SIS420-012024/blob/main/Laboratorios/Laboratorio%203/Laboratorio%203.ipynb?hl=es) -->\n",
    "  \n",
    " <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se hizo uso del para entrenar el modelo de **Regresion Logistica one vs all**\n",
    "\n",
    " A lo largo de los años, se han recopilado datos de \"INVESTIGACIÓN INCENDIOS FORESTALES EN TURQUÍA\" Haciendo uso del siguiente dataset: [THE CONNECTION BETWEEN PLANTS AND HOTPOINTS/TURKEY](https://www.kaggle.com/datasets/brsdincer/the-connection-between-plants-and-hotpointsturkey).\n",
    "\n",
    "El archivo `dataset_01.csv` CONTIENE LOS DATOS NECESARIOS PARA INVESTIGAR LOS ÚLTIMOS INCENDIOS FORESTALES EN TURQUÍA.\n",
    "\n",
    "Datos sobre los recientes incendios forestales en Turquía, publicados con permiso del Portal de la NASA.\n",
    "Los datos se crearon a partir de los puntos calientes y se obtuvieron del satélite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clasificacion multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos todas las librerias necesarias para el manejo del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando la libreria os para manejos de directorios\n",
    "import os\n",
    "\n",
    "# Computacion vectorial y cientifica para python\n",
    "import numpy as np\n",
    "\n",
    "#importamos pandas para el manejo del dataset, y separarlos dentro de una matriz\n",
    "import pandas as pd\n",
    "\n",
    "#esta tabulate nos sirve para hacer tablas\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Librerias para graficación (trazado de gráficos)\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # -> Necesario para graficar superficies 3D\n",
    "\n",
    "#Para separa el 20% y 80%\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modulo de optimizacion en scipy\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cargamos los datos del dataset\n",
    "cargamos los datos haciendo el uso de la libreria **Pandas** que  es una herramienta poderosa y versátil utilizada para manipulación y análisis de datos. Ofrece estructuras de datos flexibles y eficientes para trabajar con datos tabulares, como hojas de cálculo en Excel o tablas SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>BRIGHTNESS</th>\n",
       "      <th>SCAN</th>\n",
       "      <th>TRACK</th>\n",
       "      <th>ACQ_DATE</th>\n",
       "      <th>ACQ_TIME</th>\n",
       "      <th>SATELLITE</th>\n",
       "      <th>CONFIDENCE</th>\n",
       "      <th>BRIGHT_T31</th>\n",
       "      <th>FRP</th>\n",
       "      <th>DAYNIGHT</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>Katman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35.35940</td>\n",
       "      <td>306.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.40</td>\n",
       "      <td>11/1/2000</td>\n",
       "      <td>815</td>\n",
       "      <td>Terra</td>\n",
       "      <td>60</td>\n",
       "      <td>294.40</td>\n",
       "      <td>13.20</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Sürekli Sulanan Alanlar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.17860</td>\n",
       "      <td>307.80</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11/2/2000</td>\n",
       "      <td>857</td>\n",
       "      <td>Terra</td>\n",
       "      <td>66</td>\n",
       "      <td>296.00</td>\n",
       "      <td>5.80</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Sulanmayan Tarım Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.04440</td>\n",
       "      <td>305.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11/2/2000</td>\n",
       "      <td>857</td>\n",
       "      <td>Terra</td>\n",
       "      <td>46</td>\n",
       "      <td>293.50</td>\n",
       "      <td>4.30</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Sulanmayan Tarım Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.19090</td>\n",
       "      <td>310.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11/2/2000</td>\n",
       "      <td>857</td>\n",
       "      <td>Terra</td>\n",
       "      <td>69</td>\n",
       "      <td>296.10</td>\n",
       "      <td>7.30</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Sulanmayan Tarım Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.41720</td>\n",
       "      <td>307.90</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11/2/2000</td>\n",
       "      <td>857</td>\n",
       "      <td>Terra</td>\n",
       "      <td>0</td>\n",
       "      <td>290.40</td>\n",
       "      <td>8.00</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>Limanlar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211294</th>\n",
       "      <td>43.69891</td>\n",
       "      <td>323.01</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.81</td>\n",
       "      <td>8/9/2021</td>\n",
       "      <td>928</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>35</td>\n",
       "      <td>295.72</td>\n",
       "      <td>82.09</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitki Değişim Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211295</th>\n",
       "      <td>43.68993</td>\n",
       "      <td>322.73</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.81</td>\n",
       "      <td>8/9/2021</td>\n",
       "      <td>928</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>53</td>\n",
       "      <td>295.43</td>\n",
       "      <td>84.54</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Geniş Yapraklı Ormanlar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211296</th>\n",
       "      <td>43.68782</td>\n",
       "      <td>324.40</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.95</td>\n",
       "      <td>8/9/2021</td>\n",
       "      <td>1106</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>50</td>\n",
       "      <td>307.05</td>\n",
       "      <td>90.94</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitki Değişim Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211297</th>\n",
       "      <td>43.69128</td>\n",
       "      <td>324.88</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.95</td>\n",
       "      <td>8/9/2021</td>\n",
       "      <td>1106</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>50</td>\n",
       "      <td>307.00</td>\n",
       "      <td>95.24</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitki Değişim Alanları</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211298</th>\n",
       "      <td>27.47067</td>\n",
       "      <td>328.73</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>8/9/2021</td>\n",
       "      <td>1106</td>\n",
       "      <td>Aqua</td>\n",
       "      <td>73</td>\n",
       "      <td>311.95</td>\n",
       "      <td>9.34</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>Endüstriyel ve Ticari Birimler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211299 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LONGITUDE  BRIGHTNESS  SCAN  TRACK   ACQ_DATE  ACQ_TIME SATELLITE  \\\n",
       "0        35.35940      306.00  2.10   1.40  11/1/2000       815     Terra   \n",
       "1        35.17860      307.80  1.10   1.00  11/2/2000       857     Terra   \n",
       "2        35.04440      305.30  1.10   1.00  11/2/2000       857     Terra   \n",
       "3        35.19090      310.00  1.10   1.00  11/2/2000       857     Terra   \n",
       "4        31.41720      307.90  1.10   1.00  11/2/2000       857     Terra   \n",
       "...           ...         ...   ...    ...        ...       ...       ...   \n",
       "211294   43.69891      323.01  3.75   1.81   8/9/2021       928      Aqua   \n",
       "211295   43.68993      322.73  3.75   1.81   8/9/2021       928      Aqua   \n",
       "211296   43.68782      324.40  4.50   1.95   8/9/2021      1106      Aqua   \n",
       "211297   43.69128      324.88  4.50   1.95   8/9/2021      1106      Aqua   \n",
       "211298   27.47067      328.73  1.07   1.03   8/9/2021      1106      Aqua   \n",
       "\n",
       "        CONFIDENCE  BRIGHT_T31    FRP DAYNIGHT  TYPE  \\\n",
       "0               60      294.40  13.20        D     0   \n",
       "1               66      296.00   5.80        D     0   \n",
       "2               46      293.50   4.30        D     0   \n",
       "3               69      296.10   7.30        D     0   \n",
       "4                0      290.40   8.00        D     2   \n",
       "...            ...         ...    ...      ...   ...   \n",
       "211294          35      295.72  82.09        D     0   \n",
       "211295          53      295.43  84.54        D     0   \n",
       "211296          50      307.05  90.94        D     0   \n",
       "211297          50      307.00  95.24        D     0   \n",
       "211298          73      311.95   9.34        D     0   \n",
       "\n",
       "                                Katman  \n",
       "0              Sürekli Sulanan Alanlar  \n",
       "1            Sulanmayan Tarım Alanları  \n",
       "2            Sulanmayan Tarım Alanları  \n",
       "3            Sulanmayan Tarım Alanları  \n",
       "4                             Limanlar  \n",
       "...                                ...  \n",
       "211294         Bitki Değişim Alanları   \n",
       "211295        Geniş Yapraklı Ormanlar   \n",
       "211296         Bitki Değişim Alanları   \n",
       "211297         Bitki Değişim Alanları   \n",
       "211298  Endüstriyel ve Ticari Birimler  \n",
       "\n",
       "[211299 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Haciendo uso de la libreria pandas para leer el dataset, delimitado por \",\"\n",
    "dataset = pd.read_csv('dataset_01.csv', delimiter=',')\n",
    "\n",
    "#eliminamos la primera columan que seria nuestro \"id\" ya que no nos sirve para el analisis\n",
    "dataset = dataset.drop(dataset.columns[0], axis=1)\n",
    "\n",
    "#ahora tenemos un dataframe\n",
    "#Imprimimos en una tabla el dataset para hacer un analisis mas claro.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostrando la tabla nos damos cuenta que nuestra Y a predicir es el **Type**, para tener una mejor vision de la cantidad de clases que existe, se hizo el siguiente codigo:\n",
    "\n",
    "donde separamos nuestra columna Y con `value_counts()` este método de pandas cuenta el número de veces que aparece cada valor único en la columna **Type** del DataFrame. Devuelve una Serie pandas donde los índices son los valores únicos de la columna **Type** y los valores son el recuento de ocurrencias de cada valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la cantidad de caracteristicas es: 12\n",
      "la cantidad de clases es: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TYPE\n",
       "0    197051\n",
       "2     14163\n",
       "3        85\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#para contar cuantas clases contiene nuestra columna y\n",
    "class_counts = dataset[\"TYPE\"].value_counts()\n",
    "\n",
    "#para contar cuantas caracteristicas tiene nuestro dataset, obviamente con sin contar nuestra y, por eso lo dropeamos, tambien dropeamos la primera columna que no tiene nombre\n",
    "feactures_counts =dataset.drop(['TYPE'], axis=1)\n",
    "feactures_counts = feactures_counts.shape[1]\n",
    "\n",
    "print(f\"la cantidad de caracteristicas es: { feactures_counts}\")\n",
    "print(f'la cantidad de clases es: 4')\n",
    "\n",
    "#mostramos la cantidad de clases tiene, y en que cantidad\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Analisis del dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 4 clases diferentes(tipo de punto caliente inferido): etiquetados como 0, 1, 2 y 3.\n",
    "\n",
    "* La clase 0 parece ser la más común, seguida por la clase 2 y luego la clase 3.\n",
    "* El recuento de cada clase es el siguiente:\n",
    "    - Clase 0: 197051 puntos.\n",
    "    - Clase 2: 14163 puntos.\n",
    "    - Clase 3: 85 puntos.\n",
    "* Este análisis de la distribución de los puntajes calientes inferidos en el conjunto de datos es útil para comprender la proporción de cada clase. Esto puede ser relevante al desarrollar un modelo de clasificación como la Regresión Logística one vs all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#almacenamos la cantidad de caracteristicas en una variable\n",
    "input_layer_size  = feactures_counts;\n",
    "\n",
    "#almacenamos la cantidad de clases en una variable\n",
    "\n",
    "#para este caso no es necesario cambiar los valores de la ultima clase, ya que este cuenta con 0,1,2,3,\n",
    "# en caso de contener 1,2,3, 4  o simplememte suma 1 a la cantidad de clases.\n",
    "num_labels = 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Separacion de datos\n",
    "Se debe tomar en cuenta que cada clase tiene su propia cantidad, por lo cual separarlos directamente en un 80% para entrenamiento y un 20% para test no seria tan efectivo, ya que puede que en el 80% hay mas datos de una clase que las otras, provocando que nuestro modelo no conozca mucho sobre esa clase, por lo cual debe separarse un 80% para entrenamiento y un 20% para pruebas de cada clase.\n",
    "\n",
    "Para este hacemos uso de la libreria **train_test_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos uso del DataFrame llamado 'dataset' que contiene nuestros datos datos\n",
    "# y es la columna que contiene las etiquetas de clase (en este caso, 'type')\n",
    "\n",
    "#creamos una variable temporal que contentra toda la columna de 'TYPE'\n",
    "y_temp = dataset['TYPE']\n",
    "\n",
    "# Para la clase 0\n",
    "\n",
    "#donde y_temp es igual a 0, separamos los datos en train_class_0, test_class_0\n",
    "data_class_0 = dataset[y_temp == 0]\n",
    "train_class_0, test_class_0 = train_test_split(data_class_0, test_size=0.2, random_state=42)\n",
    "\n",
    "# Para la clase 1\n",
    "\n",
    "# #donde y_temp es igual a 1, separamos los datos en train_class_1, test_class_1\n",
    "# data_class_1 = dataset[y_temp == 1]\n",
    "# train_class_1, test_class_1 = train_test_split(data_class_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Para la clase 2\n",
    "\n",
    "#donde y_temp es igual a 2, separamos los datos en train_class_2, test_class_2\n",
    "data_class_2 = dataset[y_temp == 2]\n",
    "train_class_2, test_class_2 = train_test_split(data_class_2, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Para la clase 3\n",
    "\n",
    "#donde y_temp es igual a 3, separamos los datos en train_class_3, test_class_3\n",
    "data_class_3 = dataset[y_temp == 3]\n",
    "train_class_3, test_class_3 = train_test_split(data_class_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "haciendo conteo de separacion de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para la clase 0 tenemos una cantidad de: 197051 donde el 80% es: 157640 y el 20% es: 39411\n",
      "Para la clase 1 tenemos una cantidad de: 14163 donde el 80% es: 11330 y el 20% es: 2833\n",
      "Para la clase 2 tenemos una cantidad de: 85 donde el 80% es: 68 y el 20% es: 17\n",
      "La cantidad total de datos es: 211299\n"
     ]
    }
   ],
   "source": [
    "print(f\"Para la clase 0 tenemos una cantidad de: { data_class_0.shape[0]} donde el 80% es: {train_class_0.shape[0]} y el 20% es: {test_class_0.shape[0]}\")\n",
    "print(f\"Para la clase 1 tenemos una cantidad de: { data_class_2.shape[0]} donde el 80% es: {train_class_2.shape[0]} y el 20% es: {test_class_2.shape[0]}\")\n",
    "print(f\"Para la clase 2 tenemos una cantidad de: { data_class_3.shape[0]} donde el 80% es: {train_class_3.shape[0]} y el 20% es: {test_class_3.shape[0]}\")\n",
    "print(f\"La cantidad total de datos es: {dataset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos cada uno en sus X_train, y_train, X_test y y_test respectivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para la parte de entrenamiento, separamos las caracteristicas de la etiqueta\n",
    "\n",
    "#para la clase 0\n",
    "X_train_class_0 = train_class_0.drop(['TYPE'], axis=1)\n",
    "y_train_class_0 = train_class_0['TYPE']\n",
    "\n",
    "#para la clase 1\n",
    "X_train_class_2 = train_class_2.drop(['TYPE'], axis=1)\n",
    "y_train_class_2 = train_class_2['TYPE']\n",
    "\n",
    "#para la clase 2\n",
    "X_train_class_3 = train_class_3.drop(['TYPE'], axis=1)\n",
    "y_train_class_3 = train_class_3['TYPE']\n",
    "\n",
    "#ahora para la parte de pruebas, separamos las caracteristicas de la etiqueta\n",
    "#para la clase 0\n",
    "X_test_class_0 = test_class_0.drop(['TYPE'], axis=1)\n",
    "y_test_class_0 = test_class_0['TYPE']\n",
    "\n",
    "#para la clase 1\n",
    "X_test_class_2 = test_class_2.drop(['TYPE'], axis=1)\n",
    "y_test_class_2 = test_class_2['TYPE']\n",
    "\n",
    "#para la clase 2\n",
    "X_test_class_3 = test_class_3.drop(['TYPE'], axis=1)\n",
    "y_test_class_3 = test_class_3['TYPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora unimos todos en una sola matriz para X y y:\n",
    "pero luego debemos mezclar los datos, haciendo uso de ``np.random.permutation(len(X))`` genera un arreglo de índices permutados aleatoriamente.\n",
    "Luego, estos índices se usan para reorganizar tanto las características como las etiquetas de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.3184 325.0 1.1 ... 13.2 'D' 'Sürekli Sulanan Alanlar']\n",
      " [33.1177 322.3 1.1 ... 21.3 'D' 'Sulanmayan Tarım Alanları']\n",
      " [27.7339 333.2 1.3 ... 17.9 'D' 'Sürekli Sulanan Alanlar']\n",
      " ...\n",
      " [31.9552 302.1 1.7 ... 7.0 'D' 'Sulanmayan Tarım Alanları']\n",
      " [40.875 319.3 1.1 ... 8.7 'D' 'Sürekli Sulanan Alanlar']\n",
      " [31.851 335.2 1.0 ... 17.8 'D' 'Sulanmayan Tarım Alanları']]\n"
     ]
    }
   ],
   "source": [
    "#separando los datos de entrenamiento y pruebas\n",
    "\n",
    "#para los datos de entrenamiento\n",
    "X_train = pd.concat([X_train_class_0, X_train_class_2, X_train_class_3]).values\n",
    "y_train = pd.concat([y_train_class_0, y_train_class_2, y_train_class_3]).values\n",
    "\n",
    "\n",
    "indices_train = np.random.permutation(len(X_train))\n",
    "X_train = X_train[indices_train]\n",
    "y_train = y_train[indices_train]\n",
    "m_train = len(y_train)\n",
    "\n",
    "#para los datos de pruebas\n",
    "X_test = pd.concat([X_test_class_0, X_test_class_2, X_test_class_3]).values\n",
    "y_test = pd.concat([y_test_class_0, y_test_class_2, y_test_class_3]).values\n",
    "\n",
    "indices_test = np.random.permutation(len(X_test))\n",
    "X_test = X_test[indices_test]\n",
    "y_test = y_test[indices_test]\n",
    "X=X_test\n",
    "y=y_test\n",
    "m_test = len(y_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos algunos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    X[:,0]   X[:, 1]   X[:, 2]   X[:, 3]   X[:, 4]   X[:, 5]   X[:, 6]   X[:, 7]   X[:, 8]   X[:, 9]  X[:, 10]  X[:, 11]         Y\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   32.3184   325.000      1.10       1.010/11/200     1110.0Aqua              72    308.30     13.20D         Sürekli Sulanan Alan0         \n",
      "   33.1177   322.300      1.10       1.02/12/2018      851.0Terra             82    289.10     21.30D         Sulanmayan Tarım Ala0         \n",
      "   27.7339   333.200      1.30       1.18/17/2003     1057.0Aqua              56    321.20     17.90D         Sürekli Sulanan Alan0         \n",
      "   28.2703   304.700      1.20       1.111/18/200     1058.0Aqua              60    294.00      6.20D         Sürekli Sulanan Alan0         \n",
      "   27.8197   319.100      1.10       1.110/3/2006     1107.0Aqua              69    306.30     11.20D         Sürekli Sulanan Alan0         \n",
      "   34.5870   334.800      2.50       1.57/7/2013      1126.0Aqua              78    309.20     57.40D         Sulanmayan Tarım Ala0         \n",
      "   35.2421   341.300      1.00       1.08/29/2008      839.0Terra             88    307.10     33.40D         Sürekli Sulanan Alan0         \n",
      "   40.4848   301.100      1.20       1.18/24/2003     2304.0Aqua              32    291.00      4.80N         Sürekli Sulanan Alan0         \n",
      "   34.8583   321.600      1.30       1.19/20/2007     1106.0Aqua              49    309.70      9.90D         Sürekli Sulanan Alan2         \n",
      "   31.8911   325.630      3.06       1.78/3/2021      1145.0Aqua              63    307.27     57.88D         İğne Yapraklı Ormanl0         \n",
      " \n",
      "El 80% de ejemplos para entrenamiento son la cantidad de: 169038 de ejemplos\n",
      "El 20% de ejemplos para pruebas son la cantidad de: 42261 de ejemplos\n"
     ]
    }
   ],
   "source": [
    "print('{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]','Y'\n",
    "))\n",
    "print('-' * 200)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:10.4f}{:10.3f}{:10.2f}{:10.1f}{:10.9s}{:10.1f}{:10.10s}{:10d}{:10.2f}{:10.2f}{:10.2s}{:10.20s}{:10.10s}'.format(\n",
    "        X_train[i, 0],\n",
    "        X_train[i, 1],\n",
    "        X_train[i, 2],\n",
    "        X_train[i, 3],\n",
    "        X_train[i, 4],\n",
    "        X_train[i, 5],\n",
    "        X_train[i, 6],\n",
    "        X_train[i, 7],\n",
    "        X_train[i, 8],\n",
    "        X_train[i, 9],\n",
    "        X_train[i, 10],\n",
    "        X_train[i, 11],\n",
    "        str(y_train[i])\n",
    "    ))\n",
    "\n",
    "\n",
    "#mostramos la cantidad de ejemplos\n",
    "print(\" \")\n",
    "print('El 80% de ejemplos para entrenamiento son la cantidad de: {:.0f} de ejemplos'.format( len(X_train)))\n",
    "print('El 20% de ejemplos para pruebas son la cantidad de: {:.0f} de ejemplos'.format(  len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalización de caracteristicas\n",
    "\n",
    "Al visualizar los datos se puede observar que las caracteristicas tienen diferentes magnitudes, por lo cual se debe transformar cada valor en una escala de valores similares, esto con el fin de que el descenso por el gradiente pueda converger mas rapidamente. Se aplica la normalizacion esto debido a que los datos de las X estan a diferentes escalas.\n",
    "\n",
    "Hacemos el uso de la siguiente funcion para normalizar los datos de las columnas X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    X_norm = X.copy()\n",
    "\n",
    "    #creamos un array de ceros con una longitud igual al número de columnas en el array X. La variable mu y sigma se inicializa como este array de ceros.\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "\n",
    "    #Creamos el promedio de cada columna de X\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    sigma[sigma == 0] = 1\n",
    "    #normalizamos los datos con la siguiente formula\n",
    "    X_norm = (X - mu) / sigma\n",
    "\n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando las caracteristicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_norm, mu, sigma\u001b[38;5;241m=\u001b[39m featureNormalize(X)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_norm)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# imprimir todos las X_norm de datos solo 10\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_norm, mu, sigma= featureNormalize(X)\n",
    "print(X_norm)\n",
    "\n",
    "# imprimir todos las X_norm de datos solo 10\n",
    "print('{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}{:>10s}'.format(\n",
    "    'X[:,0]', 'X[:, 1]', 'X[:, 2]', 'X[:, 3]', 'X[:, 4]', 'X[:, 5]', 'X[:, 6]', 'X[:, 7]', 'X[:, 8]', 'X[:, 9]', 'X[:, 10]', 'X[:, 11]','Y'\n",
    "))\n",
    "print('-' * 200)\n",
    "\n",
    "for i in range(10):\n",
    "    print('{:10.4f}{:10.3f}{:10.2f}{:10.1f}{:10.9s}{:10.1f}{:10.10s}{:10d}{:10.2f}{:10.2f}{:10.2s}{:10.20s}{:10.10s}'.format(\n",
    "        X_norm[i, 0],\n",
    "        X_norm[i, 1],\n",
    "        X_norm[i, 2],\n",
    "        X_norm[i, 3],\n",
    "        X_norm[i, 4],\n",
    "        X_norm[i, 5],\n",
    "        X_norm[i, 6],\n",
    "        X_norm[i, 7],\n",
    "        X_norm[i, 8],\n",
    "        X_norm[i, 9],\n",
    "        X_norm[i, 10],\n",
    "        X_norm[i, 11],\n",
    "        y_train[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creacion de la funcion Sigmoide\n",
    "También conocida como la función logística, es una función matemática que toma cualquier número real como entrada y devuelve un valor en el rango de 0 a 1. Donde nuestra **Z** es nuestra hipotesis.\n",
    "\n",
    "Creando la funcion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Funcion de costo\n",
    "Aplicando la teoria donde nuestra funcion recibira parametros como Theta, x, y y lamda_, donde lamda_ es nuestro parametro de regularizacion.\n",
    "\n",
    "Donde la funcion nos devuelve un costo y nuestro gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrCostFunction(theta, X, y, lambda_):\n",
    "    #creamos una variable m que contiene la longitud de y\n",
    "    m = y.size\n",
    "\n",
    "    # convierte las etiquetas a valores enteros si son boleanos\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "\n",
    "    #inicializamos J y grad\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    #calculamos h haciendo uso de la funcion sigmoid, donde h es nuestra hipotesis\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "\n",
    "    temp = theta\n",
    "    temp[0] = 0\n",
    "\n",
    "    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n",
    "\n",
    "    grad = (1 / m) * (h - y).dot(X)\n",
    "    \n",
    "    grad = grad + (lambda_ / m) * temp\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Clasificacion One-vs-All\n",
    "Aqui estariamos usando un ciclo for para iterar sobre cada una de las clases, luego haciendo uso de `optimize.minimize` que es un método de la biblioteca *scipy* que encuentra el mínimo de una función. En este caso, se trata de minimizar la función de costos de regresión logística `(lrCostFunction)`.\n",
    "\n",
    "Los parámetros iniciales ``(initial_theta)``.\n",
    "Una tupla que contiene los datos de entrenamiento ``(X)``, las etiquetas ``(y == c)``, y el parámetro de regularización ``(lambda_)``.\n",
    "``jac=True`` indica que la función de coste devuelve tanto el coste como el gradiente.\n",
    "El método de optimización ``('CG' significa Gradiente Conjugado).``\n",
    "El diccionario de opciones ``(options)``  que se establece en 1000 para limitar el número máximo de iteraciones del optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lambda_):\n",
    "    # algunas variables utiles\n",
    "    #m es la longitud de y, n es la cantidad de columnas en X\n",
    "    m, n = X.shape\n",
    "\n",
    "    # inicializamos la matriz de thetas con ceros, y con n+1 columnas que seria nuestro sesgo\n",
    "    all_theta = np.zeros((num_labels, n + 1))\n",
    "\n",
    "    # Agrega unos a la matriz X\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "\n",
    "    # iteramos sobre cada etiqueta (clase) y entrenamos un clasificador\n",
    "    for c in np.arange(num_labels):\n",
    "        initial_theta = np.zeros(n + 1)\n",
    "        options = {'maxiter': 1000}\n",
    "        res = optimize.minimize(lrCostFunction,\n",
    "                                initial_theta,\n",
    "                                (X, (y == c), lambda_),\n",
    "                                jac=True,\n",
    "                                method='CG',\n",
    "                                options=options)\n",
    "\n",
    "        all_theta[c] = res.x\n",
    "\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos nuestra lambda con valor de *0.005*, usamos la funcion de `oneVsAll` donde pasamos los parametros de *X_norm*, *num_labels* que seria la cantidad de clases que tenemos, y nuestro *lanbda* para asi obtener nuestros Thetas para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m\n\u001b[1;32m----> 2\u001b[0m all_theta \u001b[38;5;241m=\u001b[39m oneVsAll(X_norm, y_train, num_labels, lambda_)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_theta\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_norm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "lambda_ = 0.005\n",
    "all_theta = oneVsAll(X_norm, y_train, num_labels, lambda_)\n",
    "print(all_theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetha 0  para la clase 0: -1.081543148619021  ;  Thetha 0  para la clase 1: 0.14285348465754039  ;  Thetha 0  para la clase 2: -2.371132391239196\n",
      "Thetha 1  para la clase 0: 0.0014111350123358936  ;  Thetha 1  para la clase 1: 0.016860051168269403  ;  Thetha 1  para la clase 2: -0.03736392896826251\n",
      "Thetha 2  para la clase 0: -0.06071940308801994  ;  Thetha 2  para la clase 1: 0.007170383640216165  ;  Thetha 2  para la clase 2: 0.04980243075604156\n",
      "Thetha 3  para la clase 0: 0.007170501633783687  ;  Thetha 3  para la clase 1: -0.0011672846659346171  ;  Thetha 3  para la clase 2: -0.014865592155955183\n",
      "Thetha 4  para la clase 0: -0.02492779318261062  ;  Thetha 4  para la clase 1: 0.0211529044503502  ;  Thetha 4  para la clase 2: -0.001077602282414906\n",
      "Thetha 5  para la clase 0: -0.09457652584528718  ;  Thetha 5  para la clase 1: -0.004293661515637002  ;  Thetha 5  para la clase 2: 0.1232058635687482\n",
      "Thetha 6  para la clase 0: -0.020546861565206725  ;  Thetha 6  para la clase 1: 0.15972877816273517  ;  Thetha 6  para la clase 2: -0.044970750758277216\n",
      "Thetha 7  para la clase 0: 0.15747466010988934  ;  Thetha 7  para la clase 1: 0.025964032462834335  ;  Thetha 7  para la clase 2: -0.6243272857000183\n",
      "Thetha 8  para la clase 0: 0.545793250920604  ;  Thetha 8  para la clase 1: -0.20729124897517678  ;  Thetha 8  para la clase 2: -0.5822066121958983\n",
      "Thetha 9  para la clase 0: 0.062129784334892094  ;  Thetha 9  para la clase 1: 0.05664267679436485  ;  Thetha 9  para la clase 2: -0.21385356409212236\n",
      "Thetha 10  para la clase 0: -0.025236677878916376  ;  Thetha 10  para la clase 1: 0.025297882804004895  ;  Thetha 10  para la clase 2: -0.017037019837464605\n",
      "Thetha 11  para la clase 0: 0.30854462906925695  ;  Thetha 11  para la clase 1: -0.005075786279944824  ;  Thetha 11  para la clase 2: -0.6243381000453205\n",
      "Thetha 12  para la clase 0: 0.0007755589340439173  ;  Thetha 12  para la clase 1: 0.010405685022179981  ;  Thetha 12  para la clase 2: -0.01767986691580968\n",
      "Thetha 13  para la clase 0: 0.032423772488244254  ;  Thetha 13  para la clase 1: -0.029571678455976948  ;  Thetha 13  para la clase 2: 0.015414344875980202\n",
      "Thetha 14  para la clase 0: 0.004828484754207329  ;  Thetha 14  para la clase 1: -0.0007042485800947989  ;  Thetha 14  para la clase 2: -0.008176170946479586\n",
      "Thetha 15  para la clase 0: -0.2185688080824547  ;  Thetha 15  para la clase 1: 0.7232273056262929  ;  Thetha 15  para la clase 2: -0.9664257582457269\n",
      "Thetha 16  para la clase 0: 0.010831979809599213  ;  Thetha 16  para la clase 1: 0.12282938955543743  ;  Thetha 16  para la clase 2: -0.42206063803997396\n",
      "Thetha 17  para la clase 0: 0.010687326620320004  ;  Thetha 17  para la clase 1: -0.006597009660749987  ;  Thetha 17  para la clase 2: -0.007148585989304377\n",
      "Thetha 18  para la clase 0: -0.11194289822659181  ;  Thetha 18  para la clase 1: -0.006518559479318166  ;  Thetha 18  para la clase 2: 0.13038108022686581\n",
      "Thetha 19  para la clase 0: 0.02823308075016595  ;  Thetha 19  para la clase 1: 0.1117625859581561  ;  Thetha 19  para la clase 2: -0.2213360827634224\n",
      "Thetha 20  para la clase 0: -0.009421480398080011  ;  Thetha 20  para la clase 1: 0.007029768107026731  ;  Thetha 20  para la clase 2: -0.0036932736559365134\n",
      "Thetha 21  para la clase 0: -0.0010918752579677055  ;  Thetha 21  para la clase 1: -0.009778258357693012  ;  Thetha 21  para la clase 2: 0.022259438222133845\n",
      "Thetha 22  para la clase 0: 0.16311147232544623  ;  Thetha 22  para la clase 1: -0.04467713812481887  ;  Thetha 22  para la clase 2: -0.18493080358651975\n",
      "Thetha 23  para la clase 0: 0.1275179508368398  ;  Thetha 23  para la clase 1: 0.005893736798693515  ;  Thetha 23  para la clase 2: -0.19914752573399266\n"
     ]
    }
   ],
   "source": [
    "for i in range(all_theta.shape[1]):\n",
    "    print(\"Thetha\",i,\" para la clase 0:\", all_theta[0,i],\" ; \", \"Thetha\",i,\" para la clase 1:\", all_theta[1,i],\" ; \", \"Thetha\",i,\" para la clase 2:\", all_theta[2,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Prediccion One-vs-all\n",
    "Aqui creamos solo la funcion, donde mandamos los parametros de `all_theta` y la `X` que en este caso puede ser las X de prueba, pero antes deben de estar normalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "    m = X.shape[0];\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "    p = np.zeros(m)\n",
    "\n",
    "    # añadimos unos a la matriz de X\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Validaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este caso de validacion se hizo uso de la funcion de `predictOneVsAll()` creada anteriormente, los siguiente fue normalizar nuestra `X_test` que son el 20% para pruebas, ahora se procedio a normalizarlo haciendo uso de *mu* y *sigma* calculado anteriormente en la funcion de normalizacion.\n",
    "\n",
    "luego se hace uso de `np.mean` donde nos calcula el promedio de los valores booleanos en el array resultante de la comparación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Con los datos de entrenamiento\n",
    "Para este caso se uso los datos de `X_norm` usando la funcion de `predictOneVsAll()`, viendo el resultado de precision nos da un **62%** el cual esta muy lejos de ser un buen modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision del conjunto de entrenamiento: 62.00%\n"
     ]
    }
   ],
   "source": [
    "# print(X_test.shape)\n",
    "pred_test = predictOneVsAll(all_theta, X_norm)\n",
    "print('Precision del conjunto de entrenamiento: {:.2f}%'.format(np.mean(pred_test == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 con los datos de prueba\n",
    "Para este caso se uso los datos de `X_test`, luego se procedio a normalizar los datos. \n",
    "\n",
    "Usando la funcion de `predictOneVsAll()`, viendo el resultado de precision nos da un **62.24%** , un poco en diferencia a la anterior prediccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision del conjunto de entrenamiento: 62.24%\n"
     ]
    }
   ],
   "source": [
    "X_test1 = X_test.copy()\n",
    "# print(X_test.shape)\n",
    "X_test = (X_test - mu) / sigma\n",
    "pred_train = predictOneVsAll(all_theta, X_test)\n",
    "print('Precision del conjunto de entrenamiento: {:.2f}%'.format(np.mean(pred_train == y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se puede hacer las predicciones usando directamente la funcion `sigmoid()`, y luego imprimimos 30 predicciones para ver que tanto se asemejan con nuestra Y predicha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15962, 24)\n",
      "Prediccion: 0, Real: 1\n",
      "Prediccion: 2, Real: 2\n",
      "Prediccion: 2, Real: 2\n",
      "Prediccion: 0, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 2, Real: 1\n",
      "Prediccion: 2, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 0, Real: 1\n",
      "Prediccion: 0, Real: 0\n",
      "Prediccion: 0, Real: 0\n",
      "Prediccion: 1, Real: 2\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 2\n",
      "Prediccion: 2, Real: 2\n",
      "Prediccion: 1, Real: 0\n",
      "Prediccion: 1, Real: 0\n",
      "Prediccion: 1, Real: 0\n",
      "Prediccion: 1, Real: 0\n",
      "Prediccion: 0, Real: 0\n",
      "Prediccion: 1, Real: 2\n",
      "Prediccion: 1, Real: 0\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 1, Real: 1\n",
      "Prediccion: 0, Real: 0\n",
      "Prediccion: 0, Real: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#usando el mu y sigma calculado anteriormente, para realizar la normalizacion de los datos de prueba\n",
    "X_test1 = (X_test1 - mu) / sigma\n",
    "X_test1 = np.concatenate([np.ones((len(X_test1), 1)), X_test1], axis=1)\n",
    "\n",
    "print(X_test1.shape)\n",
    "\n",
    "#para este ejemplo, vamos a mostrar las predicciones de las primeras 30 predicciones\n",
    "p = np.argmax(sigmoid(X_test1.dot(all_theta.T)), axis = 1)\n",
    "\n",
    "for i in range(30):\n",
    "    print(f\"Prediccion: {p[i]}, Real: {y_test[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
